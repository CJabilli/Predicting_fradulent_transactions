# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hV_xKZI5N-uNsumWz0h0KOVUtbLR4Uz
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from datetime import datetime
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv("/kaggle/input/fraud-detection-dataset/data2.csv")

# Convert Expiry to a Numeric Feature (Months to Expiry)
current_date = datetime.now()

def calculate_months_to_expiry(expiry):
    exp_month, exp_year = map(int, expiry.split("/"))
    expiry_date = datetime(year=2000 + exp_year, month=exp_month, day=1)
    return max(0, (expiry_date.year - current_date.year) * 12 + (expiry_date.month - current_date.month))

data['Months_to_expiry'] = data['Expiry'].apply(calculate_months_to_expiry)

# Encode 'Profession' using One-Hot Encoding
data = pd.get_dummies(data, columns=['Profession'], drop_first=True)

# Drop irrelevant or sensitive features
data = data.drop(columns=['Credit_card_number', 'Expiry'])

# Split the dataset into features and target variable
X = data.drop(columns=['Fraud'])
y = data['Fraud']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Import Classification Models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

# Define classifiers
models = [
    ("Random Forest", RandomForestClassifier(random_state=42)),
    ("Gradient Boosting", GradientBoostingClassifier(random_state=42)),
    ("AdaBoost", AdaBoostClassifier(random_state=42)),
    ("SVM", SVC(probability=True, random_state=42)),
    ("Logistic Regression", LogisticRegression(random_state=42)),
    ("Ridge Classifier", RidgeClassifier()),
    ("Perceptron", Perceptron(random_state=42)),
    ("K-Nearest Neighbors", KNeighborsClassifier()),
    ("Decision Tree", DecisionTreeClassifier(random_state=42)),
    ("Naive Bayes", GaussianNB())
]

# Train and evaluate each model
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Model: {name}")
    print(classification_report(y_test, y_pred))
    print("-" * 60)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns  # Import seaborn

# Load the dataset"C
data = pd.read_csv("C:\Users\jaballi\Desktop\data2.csv")

# Convert Expiry to a Numeric Feature (Months to Expiry)
current_date = datetime.now()

def calculate_months_to_expiry(expiry):
    exp_month, exp_year = map(int, expiry.split("/"))
    expiry_date = datetime(year=2000 + exp_year, month=exp_month, day=1)
    return max(0, (expiry_date.year - current_date.year) * 12 + (expiry_date.month - current_date.month))

data['Months_to_expiry'] = data['Expiry'].apply(calculate_months_to_expiry)

# Encode 'Profession' using One-Hot Encoding
data = pd.get_dummies(data, columns=['Profession'], drop_first=True)

# Drop irrelevant or sensitive features
data = data.drop(columns=['Credit_card_number', 'Expiry'])

# Split the dataset into features and target variable
X = data.drop(columns=['Fraud'])
y = data['Fraud']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Import Classification Models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

# Define classifiers
models = [
    ("Random Forest", RandomForestClassifier(random_state=42)),
    ("Gradient Boosting", GradientBoostingClassifier(random_state=42)),
    ("AdaBoost", AdaBoostClassifier(random_state=42)),
    ("SVM", SVC(probability=True, random_state=42)),
    ("Logistic Regression", LogisticRegression(random_state=42)),
    ("Ridge Classifier", RidgeClassifier()),
    ("Perceptron", Perceptron(random_state=42)),
    ("K-Nearest Neighbors", KNeighborsClassifier()),
    ("Decision Tree", DecisionTreeClassifier(random_state=42)),
    ("Naive Bayes", GaussianNB())
]

# Train and evaluate each model
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"Model: {name}")
    print("Confusion Matrix:")
    print(cm)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("-" * 60)

    # Plot Confusion Matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
    plt.title(f"Confusion Matrix: {name}")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()